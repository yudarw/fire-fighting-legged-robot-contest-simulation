{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(607, 9)\n",
      "   0   1   2    3    4   5   6    7       8\n",
      "0  7   9  23  127  147  96  81  172  room1A\n",
      "1  7   9  23  128  147  96  81  172  room1A\n",
      "2  8  10  25  126  145  95  80  172  room1A\n",
      "3  8  11  26  124  143  94  79  171  room1A\n",
      "4  8  11  26  123  142  94  79  171  room1A\n",
      "[[-1.68033981 -1.21086731 -2.11021578  1.81908366  1.99301288  2.71237406\n",
      "   2.2042971   1.58279124]\n",
      " [ 0.29741533  2.69523581  1.33575948  0.0846627  -0.64197118 -0.77625756\n",
      "  -0.89174533 -0.74343225]\n",
      " [-0.25376233 -0.42964669 -0.56146285 -1.36741066 -0.12398286 -0.16061668\n",
      "  -0.31381741 -0.41111461]\n",
      " [ 0.26499312 -0.06295129 -0.87121344 -1.44808141 -1.04735334 -0.98147118\n",
      "  -0.97430647 -0.76558676]\n",
      " [ 1.98337053  1.65892273  1.21960301  0.44768104 -1.02483211 -0.94042846\n",
      "  -0.9330259  -0.76558676]]\n",
      "[0 1 4 4 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the room dataset\n",
    "dataset = pd.read_csv('dataset_room.csv', header=None)\n",
    "print(dataset.shape)\n",
    "print(dataset.head())\n",
    "\n",
    "# Separate features and labels\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print a few of X_train and y_train\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multi-layer Perceptron (MLP)\n",
    "\n",
    "A Multi-layer Perceptron (MLP) is a class of feedforward artificial neural network (ANN). It consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, in the MLP is a computational unit that processes input data and passes the result to the next layer.\n",
    "\n",
    "## Key Characteristics of MLP:\n",
    "- **Feedforward Architecture**: Data flows in one direction, from input to output, without any cycles or loops.\n",
    "- **Fully Connected Layers**: Each neuron in one layer is connected to every neuron in the next layer.\n",
    "- **Activation Functions**: Non-linear functions applied to the output of each neuron, enabling the network to learn complex patterns. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "\n",
    "## How MLP Works:\n",
    "1. **Input Layer**: Receives the input data. Each neuron in this layer represents a feature of the input data.\n",
    "2. **Hidden Layers**: Intermediate layers that perform computations and feature transformations. The number of hidden layers and neurons in each layer can be adjusted based on the complexity of the problem.\n",
    "3. **Output Layer**: Produces the final output of the network. For classification tasks, this layer typically uses a softmax activation function to output probabilities for each class.\n",
    "\n",
    "## Training Process:\n",
    "- **Forward Propagation**: Input data is passed through the network, layer by layer, to generate predictions.\n",
    "- **Loss Calculation**: The difference between the predicted output and the actual target is measured using a loss function (e.g., mean squared error for regression, cross-entropy for classification).\n",
    "- **Backpropagation**: The loss is propagated backward through the network to update the weights and biases of the neurons. This is done using gradient descent optimization techniques.\n",
    "- **Iteration**: The forward and backward propagation steps are repeated for multiple iterations (epochs) until the network's performance converges to an acceptable level.\n",
    "\n",
    "MLPs are widely used for various tasks, including image and speech recognition, natural language processing, and more. They are particularly effective for problems where the relationship between input and output is complex and non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classification using a Multi-layer Perceptron (MLP) classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "# print the acacuracy of the model\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "## Overview\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. However, it is mostly used for classification problems. The goal of the SVM algorithm is to find a hyperplane in an N-dimensional space (N â€” the number of features) that distinctly classifies the data points.\n",
    "\n",
    "## How SVM Works\n",
    "1. **Hyperplane**: In SVM, a hyperplane is a decision boundary that separates the data points of different classes. The dimension of the hyperplane depends on the number of features. For example, if there are two features, the hyperplane is a line. If there are three features, the hyperplane becomes a two-dimensional plane.\n",
    "\n",
    "2. **Support Vectors**: Support vectors are the data points that are closest to the hyperplane. These points are critical in defining the position and orientation of the hyperplane. The SVM algorithm aims to maximize the margin between the support vectors of the two classes.\n",
    "\n",
    "3. **Margin**: The margin is the distance between the hyperplane and the nearest data point from either class. SVM tries to maximize this margin to ensure that the model is robust and has a lower generalization error.\n",
    "\n",
    "4. **Kernel Trick**: SVM can efficiently perform a non-linear classification using what is called the kernel trick. The kernel trick involves transforming the data into a higher-dimensional space where a linear separator can be found. Common kernels include:\n",
    "    - Linear Kernel\n",
    "    - Polynomial Kernel\n",
    "    - Radial Basis Function (RBF) Kernel\n",
    "    - Sigmoid Kernel\n",
    "\n",
    "5. **Optimization**: The SVM algorithm solves an optimization problem to find the best hyperplane. The objective is to minimize the classification error while maximizing the margin. This is typically done using techniques such as Quadratic Programming.\n",
    "\n",
    "## Steps in SVM Algorithm\n",
    "1. **Data Preparation**: Collect and preprocess the data, including normalization and handling missing values.\n",
    "2. **Choosing the Kernel**: Select an appropriate kernel function based on the problem and data characteristics.\n",
    "3. **Training the Model**: Use the training data to find the optimal hyperplane that separates the classes.\n",
    "4. **Model Evaluation**: Evaluate the model using metrics such as accuracy, precision, recall, and F1-score.\n",
    "5. **Hyperparameter Tuning**: Adjust parameters like the regularization parameter (C) and kernel parameters to improve model performance.\n",
    "6. **Prediction**: Use the trained SVM model to classify new data points.\n",
    "\n",
    "## Advantages of SVM\n",
    "- Effective in high-dimensional spaces.\n",
    "- Works well with a clear margin of separation.\n",
    "- Effective when the number of dimensions is greater than the number of samples.\n",
    "- Memory efficient as it uses a subset of training points (support vectors).\n",
    "\n",
    "## Disadvantages of SVM\n",
    "- Not suitable for large datasets as the training time is higher.\n",
    "- Less effective on noisy data with overlapping classes.\n",
    "- Requires careful tuning of parameters and selection of the kernel function.\n",
    "\n",
    "SVM is a powerful and versatile machine learning algorithm that can be used for various classification and regression tasks. Its ability to handle high-dimensional data and find a robust decision boundary makes it a popular choice in many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classification using SVM\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method used for classification, regression, and other tasks. It operates by constructing multiple decision trees during training and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "## Key Characteristics of Random Forest:\n",
    "- **Ensemble Method**: Combines multiple decision trees to improve the model's accuracy and robustness.\n",
    "- **Bootstrap Aggregation (Bagging)**: Each tree is trained on a random subset of the data, with replacement, to reduce variance.\n",
    "- **Random Feature Selection**: At each split in the tree, a random subset of features is considered, which helps in reducing correlation among trees.\n",
    "\n",
    "## How Random Forest Works:\n",
    "1. **Data Preparation**: The dataset is divided into multiple subsets using bootstrapping (random sampling with replacement).\n",
    "2. **Tree Construction**: For each subset, a decision tree is constructed. During the construction, at each node, a random subset of features is selected, and the best split is chosen from this subset.\n",
    "3. **Voting/Averaging**: For classification tasks, each tree in the forest votes for a class, and the class with the majority votes is chosen as the final prediction. For regression tasks, the average of the predictions from all trees is taken as the final prediction.\n",
    "\n",
    "## Steps in Random Forest Algorithm:\n",
    "1. **Bootstrap Sampling**: Randomly select samples from the training data with replacement to create multiple subsets.\n",
    "2. **Tree Building**: For each subset, build a decision tree:\n",
    "    - At each node, select a random subset of features.\n",
    "    - Choose the best feature and split point from this subset to split the node.\n",
    "    - Repeat the process until the maximum depth is reached or other stopping criteria are met.\n",
    "3. **Prediction**:\n",
    "    - For classification: Each tree in the forest makes a prediction, and the class with the most votes is the final prediction.\n",
    "    - For regression: Each tree makes a prediction, and the average of all predictions is the final prediction.\n",
    "\n",
    "## Advantages of Random Forest:\n",
    "- **High Accuracy**: By combining multiple trees, Random Forest often achieves higher accuracy than individual decision trees.\n",
    "- **Robustness**: Reduces overfitting by averaging multiple trees, making it more robust to noise in the data.\n",
    "- **Feature Importance**: Provides estimates of feature importance, which can be useful for understanding the data and feature selection.\n",
    "\n",
    "## Disadvantages of Random Forest:\n",
    "- **Complexity**: The model can become complex and computationally intensive with a large number of trees.\n",
    "- **Interpretability**: While individual decision trees are easy to interpret, the ensemble of many trees in a Random Forest can be more challenging to interpret.\n",
    "\n",
    "## Applications of Random Forest:\n",
    "- **Classification**: Used in various classification tasks such as spam detection, image recognition, and medical diagnosis.\n",
    "- **Regression**: Applied in regression tasks like predicting house prices, stock market trends, and more.\n",
    "- **Feature Selection**: Helps in identifying important features in the dataset.\n",
    "\n",
    "Random Forest is a powerful and versatile machine learning algorithm that can handle both classification and regression tasks. Its ability to reduce overfitting and provide high accuracy makes it a popular choice in many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classififcation using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classification using K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classification using Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      room1A       1.00      1.00      1.00        28\n",
      "      room1B       1.00      1.00      1.00        25\n",
      "       room2       1.00      1.00      1.00        23\n",
      "       room3       1.00      1.00      1.00        21\n",
      "       room4       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00       122\n",
      "   macro avg       1.00      1.00      1.00       122\n",
      "weighted avg       1.00      1.00      1.00       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Room classification using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
